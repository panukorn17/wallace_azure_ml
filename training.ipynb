{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "import azure.ai.ml\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.ml import command, Input\n",
    "from azure.ai.ml.entities import (\n",
    "    AzureBlobDatastore,\n",
    "    AzureFileDatastore,\n",
    "    AzureDataLakeGen1Datastore,\n",
    "    AzureDataLakeGen2Datastore,\n",
    ")\n",
    "from azureml.core import Environment\n",
    "import io\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import _pickle as cPickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import keras\n",
    "import mlflow\n",
    "import joblib\n",
    "import json\n",
    "from sys import path\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlflow.models.signature import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\", \"r\") as f:\n",
    "    config = json.load(f)\n",
    "subscription_id = config[\"azure_ml_subscription_ID\"]\n",
    "resource_group = config[\"resource_group\"]\n",
    "workspace = config[\"workspace\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/DL_data_next_1_station_2016_dm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterating the columns\n",
    "column_headers = list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildPreprocessorPipeline(df):\n",
    "    #used\n",
    "    #63,0  is cumulative arr delay\n",
    "    #62,1  is cumulative dep delay\n",
    "    #18,2 is day\n",
    "    #58,3 is deviation from arrival\n",
    "    #59,4 is deviation from departure\n",
    "    #34,5 is direction\n",
    "    #35,6 is dwell time curr station\n",
    "    #39,7 is dwell time next station average\n",
    "    #31,8 is (is destination?)\n",
    "    #30,9 is (is origin?)\n",
    "    #19,10 is month\n",
    "    #27,11 is number of stops\n",
    "    #33,12 is order of departure\n",
    "    #32,13 is order of journey\n",
    "    #28,14 is origin departure period\n",
    "    #56,15 is travel time next station average\n",
    "    #46,16 is travel time prev station\n",
    "    numeric_features = ['cumulative_arrival_delay',\n",
    "                        'cumulative_departure_delay',\n",
    "                        'deviation_from_arrival',\n",
    "                        'deviation_from_departure',\n",
    "                        'dwell_time_curr_station',\n",
    "                        'dwell_time_next_1_station_average',\n",
    "                        'number_stops',\n",
    "                        'order_of_departure',\n",
    "                        'order_of_journey',\n",
    "                        'origin_departure_period',\n",
    "                        'travel_time_next_1_station_average',\n",
    "                        'travel_time_prev_station']\n",
    "\n",
    "    categorical_features = ['day',\n",
    "                            'month',\n",
    "                            'direction',\n",
    "                            'is_destination',\n",
    "                            'is_origin']\n",
    "    \n",
    "    #encoding categorical data\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[(\"encoder\", OneHotEncoder(handle_unknown = \"ignore\"))\n",
    "        ]\n",
    "    )\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", StandardScaler(), numeric_features),\n",
    "            (\"cat\", categorical_transformer, categorical_features),\n",
    "        ], remainder=\"drop\"\n",
    "    )\n",
    "    \n",
    "    return preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(28, input_dim=35, activation='relu'))\n",
    "    model.add(Dense(28, activation='relu'))\n",
    "    model.add(Dense(28, activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mse','mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_train(df):\n",
    "    X_raw = df.iloc[:,[63,62,18,58,59,34,35,39,31,30,19,27,33,32,28,56,46]]\n",
    "    Y_raw = df.iloc[:,60]\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X_raw, Y_raw, test_size = 0.2)\n",
    "    preprocessor = buildPreprocessorPipeline(X_train)\n",
    "\n",
    "    # estimator instance\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('regressor', KerasRegressor(build_fn=create_model, epochs=100, batch_size=64, validation_split = 0.2, verbose=1))])\n",
    "    model = clf.fit(X_train, Y_train)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(Y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(\"RMSE: %f\" % (rmse))\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "\n",
    "    # Calculate R2\n",
    "    r2 = r2_score(Y_test, y_pred)\n",
    "    print(\"R2: %f\" % (r2))\n",
    "    mlflow.log_metric(\"r2\", r2)\n",
    "\n",
    "    # Plot the predictions vs actual\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(Y_test, y_pred)\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('Predictions vs Actual')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    #start logging\n",
    "    experiment_name = \"data_next_1_station_local_compute\"\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    #start logging\n",
    "    mlflow.start_run()\n",
    "    mlflow.sklearn.autolog()\n",
    "    mlflow.log_metric(\"num_samples\", df.shape[0])\n",
    "    mlflow.log_param(\"num_features\", df.shape[1])\n",
    "    model = model_train(df)\n",
    "\n",
    "    os.makedirs('outputs', exist_ok=True)\n",
    "    model_file = os.path.join('outputs', 'dnn_model.pkl')\n",
    "    joblib.dump(value=model, filename=model_file)\n",
    "\n",
    "    # Register the model\n",
    "    # stop logging\n",
    "    mlflow.end_run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
